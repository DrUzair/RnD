Deep learning (Big Data + Neural Networks + GPUs) is overshadowing domain specific
feature engineering and making machines outsmart humans in our home ground of language and vision. 
Why are we needed at all ? 
Prepare for 'age of useless people'.

The dawn of deep learning has shown remarkable developments in AI. Consequently, engineers with skills in 'Deep Learning' achieve coveted positions in the hierarchy of knowledge workers. To shorten the 'concept to market' time, the relevant software tools and frameworks are becoming more and more abstract, hiding the intricacies of Neural Learning systems. An engineer with good understanding of APIs don't need to worry about coding (or even understanding) derivation of cost function or backpropgation of error. 

Opensource implementations of generic feature extractors With advanced APIs and Frameworks like keras and tensorflow, it seems like 

Machine Learning is domain dependent and trial-and-error descipline.
Domain-dependency: Solutions/Techniques/Insights that work for one domain (NLP) may not hold useful in other domain (Image/Video Processing)
Trial-and-Error: Every different problem need to pass through numerous iterations to find out best values for hyperparameters (learning-rate, neural network architechture, activation functions etc)

Even after getting a solution right, the real success (prediction performance) hinges of the condition the future (unseen) data should come from the same distribution as training data.

underfitting --> High bias
overfitting --> High variance
